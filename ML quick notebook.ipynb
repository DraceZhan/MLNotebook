{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is meant to be a simple SKLearn tutorial/cheatsheet notebook. Written by Drace Zhan of NYCDSA for student/public use.\n",
    "Note that the mathematics behind the models will not be covered here and the examples used will be purely for artificial purposes so there won't be any preprocessing, train-test-splits, verifying assumptions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data manipulation tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#data visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading datasets used for this notebook\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading iris from dictionary to dataframe\n",
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading boston dataset from dictionary to dataframe\n",
    "boston = datasets.load_boston()\n",
    "boston_df = pd.DataFrame(boston.data, columns= boston.feature_names)\n",
    "boston_df['target'] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Linear Regression & Logistic Regression\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  target  \n",
       "0     15.3  396.90   4.98    24.0  \n",
       "1     17.8  396.90   9.14    21.6  \n",
       "2     17.8  392.83   4.03    34.7  \n",
       "3     18.7  394.63   2.94    33.4  \n",
       "4     18.7  396.90   5.33    36.2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Model objects in Sklearn has various parameters and arguments you can pass in to the object as you create it. I highly recommend reading the documentation to understand what you can do to adjust the parameters on it. One of the more useful ones is \"n_jobs\". This argument is the amount of cores your CPU will use running the model. Setting it to -1 such as \"n_jobs = -1\" will force it to use ALL your cores. Your laptop will get quite hot during this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a Linear regression in SKlearn\n",
    "linreg_1 = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving the X & y labels in Python can be a bit strange. The thing to keep in mind is that you are trying to pass in the values of your X and not the column names themselves. I recommend first creating a list of your column names before passing it into X as the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating X, y for Boston dataset, for this toy example, we'll be using all the columns except for Y for a toy example.\n",
    "b_feat_list = boston_df.columns[0:-1]\n",
    "X = boston_df[b_feat_list]\n",
    "y = boston_df.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you create the model object, it will allow you to fit data into it and then will start training to your data. After the model finishes training, it will allow you to call on the various attributes of the now trained model. This will be elaborated further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the LinearRegression object to your data\n",
    "linreg_1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the object, you'll note it now has several attributes you can call upon. Feel free to experiment with them! I will list some of the more useful ones below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.07170557e-01,   4.63952195e-02,   2.08602395e-02,\n",
       "         2.68856140e+00,  -1.77957587e+01,   3.80475246e+00,\n",
       "         7.51061703e-04,  -1.47575880e+00,   3.05655038e-01,\n",
       "        -1.23293463e-02,  -9.53463555e-01,   9.39251272e-03,\n",
       "        -5.25466633e-01])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the coefficients of your LinearRegression object\n",
    "linreg_1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.491103280363404"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the intercept\n",
    "linreg_1.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7406077428649428"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the R2 score of your model\n",
    "linreg_1.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30.00821269,  25.0298606 ,  30.5702317 ,  28.60814055,\n",
       "        27.94288232,  25.25940048,  23.00433994,  19.5347558 ,\n",
       "        11.51696539,  18.91981483,  18.9958266 ,  21.58970854,\n",
       "        20.90534851,  19.55535931,  19.2837957 ,  19.30000174,\n",
       "        20.52889993,  16.9096749 ,  16.17067411,  18.40781636,\n",
       "        12.52040454,  17.67104565,  15.82934891,  13.80368317,\n",
       "        15.67708138,  13.3791645 ,  15.46258829,  14.69863607,\n",
       "        19.54518512,  20.87309945,  11.44806825,  18.05900412,\n",
       "         8.78841666,  14.27882319,  13.69097132,  23.81755469,\n",
       "        22.34216285,  23.11123204,  22.91494157,  31.35826216,\n",
       "        34.21485385,  28.0207132 ,  25.20646572,  24.61192851,\n",
       "        22.94438953,  22.10150945,  20.42467417,  18.03614022,\n",
       "         9.10176198,  17.20856571,  21.28259372,  23.97621248,\n",
       "        27.65853521,  24.0521088 ,  15.35989132,  31.14817003,\n",
       "        24.85878746,  33.11017111,  21.77458036,  21.08526739,\n",
       "        17.87203538,  18.50881381,  23.9879809 ,  22.54944098,\n",
       "        23.37068403,  30.36557584,  25.53407332,  21.11758504,\n",
       "        17.42468223,  20.7893086 ,  25.20349174,  21.74490595,\n",
       "        24.56275612,  24.04479519,  25.5091157 ,  23.97076758,\n",
       "        22.94823519,  23.36106095,  21.26432549,  22.4345376 ,\n",
       "        28.40699937,  26.99734716,  26.03807246,  25.06152125,\n",
       "        24.7858613 ,  27.79291889,  22.16927073,  25.89685664,\n",
       "        30.67771522,  30.83225886,  27.12127354,  27.41597825,\n",
       "        28.9456478 ,  29.08668003,  27.04501726,  28.62506705,\n",
       "        24.73038218,  35.78062378,  35.11269515,  32.25115468,\n",
       "        24.57946786,  25.59386215,  19.76439137,  20.31157117,\n",
       "        21.4353635 ,  18.53971968,  17.18572611,  20.74934949,\n",
       "        22.64791346,  19.77000977,  20.64745349,  26.52652691,\n",
       "        20.77440554,  20.71546432,  25.17461484,  20.4273652 ,\n",
       "        23.37862521,  23.69454145,  20.33202239,  20.79378139,\n",
       "        21.92024414,  22.47432006,  20.55884635,  16.36300764,\n",
       "        20.56342111,  22.48570454,  14.61264839,  15.1802607 ,\n",
       "        18.93828443,  14.0574955 ,  20.03651959,  19.41306288,\n",
       "        20.06401034,  15.76005772,  13.24771577,  17.26167729,\n",
       "        15.87759672,  19.36145104,  13.81270814,  16.44782934,\n",
       "        13.56511101,   3.98343974,  14.59241207,  12.14503093,\n",
       "         8.72407108,  12.00815659,  15.80308586,   8.50963929,\n",
       "         9.70965512,  14.79848067,  20.83598096,  18.30017013,\n",
       "        20.12575267,  17.27585681,  22.35997992,  20.07985184,\n",
       "        13.59903744,  33.26635221,  29.03938379,  25.56694529,\n",
       "        32.71732164,  36.78111388,  40.56615533,  41.85122271,\n",
       "        24.79875684,  25.3771545 ,  37.20662185,  23.08244608,\n",
       "        26.40326834,  26.65647433,  22.55412919,  24.2970948 ,\n",
       "        22.98024802,  29.07488389,  26.52620066,  30.72351225,\n",
       "        25.61835359,  29.14203283,  31.43690634,  32.9232938 ,\n",
       "        34.72096487,  27.76792733,  33.88992899,  30.99725805,\n",
       "        22.72124288,  24.76567683,  35.88131719,  33.42696242,\n",
       "        32.41513625,  34.51611818,  30.76057666,  30.29169893,\n",
       "        32.92040221,  32.11459912,  31.56133385,  40.84274603,\n",
       "        36.13046343,  32.66639271,  34.70558647,  30.09276228,\n",
       "        30.64139724,  29.29189704,  37.07062623,  42.02879611,\n",
       "        43.18582722,  22.6923888 ,  23.68420569,  17.85435295,\n",
       "        23.49543857,  17.00872418,  22.39535066,  17.06152243,\n",
       "        22.74106824,  25.21974252,  11.10601161,  24.51300617,\n",
       "        26.60749026,  28.35802444,  24.91860458,  29.69254951,\n",
       "        33.18492755,  23.77145523,  32.14086508,  29.74802362,\n",
       "        38.36605632,  39.80716458,  37.58362546,  32.39769704,\n",
       "        35.45048257,  31.23446481,  24.48478321,  33.28615723,\n",
       "        38.04368164,  37.15737267,  31.71297469,  25.26658017,\n",
       "        30.101515  ,  32.71897655,  28.42735376,  28.42999168,\n",
       "        27.2913215 ,  23.74446671,  24.11878941,  27.40241209,\n",
       "        16.32993575,  13.39695213,  20.01655581,  19.86205904,\n",
       "        21.28604604,  24.07796482,  24.20603792,  25.04201534,\n",
       "        24.91709097,  29.93762975,  23.97709054,  21.69931969,\n",
       "        37.51051381,  43.29459357,  36.48121427,  34.99129701,\n",
       "        34.80865729,  37.16296374,  40.9823638 ,  34.44211691,\n",
       "        35.83178068,  28.24913647,  31.22022312,  40.83256202,\n",
       "        39.31768808,  25.71099424,  22.30344878,  27.20551341,\n",
       "        28.51386352,  35.47494122,  36.11110647,  33.80004807,\n",
       "        35.61141951,  34.84311742,  30.35359323,  35.31260262,\n",
       "        38.79684808,  34.33296541,  40.34038636,  44.67339923,\n",
       "        31.5955473 ,  27.35994642,  20.09520596,  27.04518524,\n",
       "        27.21674397,  26.91105226,  33.43602979,  34.40228785,\n",
       "        31.83374181,  25.82416035,  24.43687139,  28.46348891,\n",
       "        27.36916176,  19.54441878,  29.11480679,  31.90852699,\n",
       "        30.77325183,  28.9430835 ,  28.88108106,  32.79876794,\n",
       "        33.20356949,  30.76568546,  35.55843485,  32.70725436,\n",
       "        28.64759861,  23.59388439,  18.5461558 ,  26.88429024,\n",
       "        23.28485442,  25.55002201,  25.48337323,  20.54343769,\n",
       "        17.61406384,  18.37627933,  24.29187594,  21.3257202 ,\n",
       "        24.88826131,  24.87143049,  22.87255605,  19.4540234 ,\n",
       "        25.11948741,  24.66816374,  23.68209656,  19.33951725,\n",
       "        21.17636041,  24.25306588,  21.59311197,  19.98766667,\n",
       "        23.34079584,  22.13973959,  21.55349196,  20.61808868,\n",
       "        20.1607571 ,  19.28455466,  22.16593919,  21.24893735,\n",
       "        21.42985456,  30.32874523,  22.04915396,  27.70610125,\n",
       "        28.54595004,  16.54657063,  14.78278261,  25.27336772,\n",
       "        27.54088054,  22.14633467,  20.46081206,  20.54472332,\n",
       "        16.88194391,  25.40066956,  14.32299547,  16.5927403 ,\n",
       "        19.63224597,  22.7117302 ,  22.19946949,  19.1989151 ,\n",
       "        22.66091019,  18.92059374,  18.22715359,  20.22444386,\n",
       "        37.47946099,  14.29172583,  15.53697148,  10.82825817,\n",
       "        23.81134987,  32.64787163,  34.61163401,  24.94604102,\n",
       "        26.00259724,   6.12085728,   0.78021126,  25.311373  ,\n",
       "        17.73465914,  20.22593282,  15.83834861,  16.83742401,\n",
       "        14.43123608,  18.47647773,  13.42427933,  13.05677824,\n",
       "         3.27646485,   8.05936467,   6.13903114,   5.62271213,\n",
       "         6.44935154,  14.20597451,  17.21022671,  17.29035065,\n",
       "         9.89064351,  20.21972222,  17.94511052,  20.30017588,\n",
       "        19.28790318,  16.33300008,   6.56843662,  10.87541577,\n",
       "        11.88704097,  17.81098929,  18.25461066,  12.99282707,\n",
       "         7.39319053,   8.25609561,   8.07899971,  19.98563715,\n",
       "        13.69651744,  19.83511412,  15.2345378 ,  16.93112419,\n",
       "         1.69347406,  11.81116263,  -4.28300934,   9.55007844,\n",
       "        13.32635521,   6.88351077,   6.16827417,  14.56933235,\n",
       "        19.59292932,  18.1151686 ,  18.52011987,  13.13707457,\n",
       "        14.59662601,   9.8923749 ,  16.31998048,  14.06750301,\n",
       "        14.22573568,  13.00752251,  18.13277547,  18.66645496,\n",
       "        21.50283795,  17.00039379,  15.93926602,  13.32952716,\n",
       "        14.48949211,   8.78366731,   4.8300317 ,  13.06115528,\n",
       "        12.71101472,  17.2887624 ,  18.73424906,  18.05271013,\n",
       "        11.49855612,  13.00841512,  17.66975577,  18.12342294,\n",
       "        17.51503231,  17.21307203,  16.48238543,  19.40079737,\n",
       "        18.57392951,  22.47833186,  15.24179836,  15.78327609,\n",
       "        12.64853778,  12.84121049,  17.17173661,  18.50906858,\n",
       "        19.02803874,  20.16441773,  19.76975335,  22.42614937,\n",
       "        20.31750314,  17.87618837,  14.3391341 ,  16.93715603,\n",
       "        16.98716629,  18.59431701,  20.16395155,  22.97743546,\n",
       "        22.45110639,  25.5707207 ,  16.39091112,  16.09765427,\n",
       "        20.52835689,  11.5429045 ,  19.20387482,  21.86820603,\n",
       "        23.47052203,  27.10034494,  28.57064813,  21.0839881 ,\n",
       "        19.4490529 ,  22.2189221 ,  19.65423066,  21.324671  ,\n",
       "        11.86231364,   8.22260592,   3.65825168,  13.76275951,\n",
       "        15.93780944,  20.62730097,  20.61035443,  16.88048035,\n",
       "        14.01017244,  19.10825534,  21.29720741,  18.45524217,\n",
       "        20.46764235,  23.53261729,  22.37869798,  27.62934247,\n",
       "        26.12983844,  22.34870269])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting with your model\n",
    "linreg_1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30.00821269,  25.0298606 ,  30.5702317 ,  28.60814055,\n",
       "        27.94288232,  25.25940048,  23.00433994,  19.5347558 ,\n",
       "        11.51696539,  18.91981483,  18.9958266 ,  21.58970854,\n",
       "        20.90534851,  19.55535931,  19.2837957 ,  19.30000174,\n",
       "        20.52889993,  16.9096749 ,  16.17067411,  18.40781636,\n",
       "        12.52040454,  17.67104565,  15.82934891,  13.80368317,\n",
       "        15.67708138,  13.3791645 ,  15.46258829,  14.69863607,\n",
       "        19.54518512,  20.87309945])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#passing in another set for your model to predict\n",
    "X_test = X[0:30]\n",
    "linreg_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how you create a Linear Regression object, Logistic Regression is much the same. Some things that are useful in the Logistic Regression arguments include \"class_weight\" to handle imbalanced classes. In addition, the Logistic Regression object will automatically include regularization (ridge by default but you can set to lasso as well). You can combat this by setting the C value to a very large number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_1 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#similar steps to \n",
    "iris_feat_list = iris_df.columns[0:-1]\n",
    "iris_X = iris_df[iris_feat_list]\n",
    "iris_y = iris_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_1.fit(iris_X, iris_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like LinearRegression, the LogisticRegression model will have many attributes you can call upon. In addition to standard prediction, the logistic regression model will also allow you to show probabilities of your predictions as well as log probabilities as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predictions\n",
    "logit_1.predict(iris_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.879682</td>\n",
       "      <td>0.120308</td>\n",
       "      <td>1.081314e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.799706</td>\n",
       "      <td>0.200263</td>\n",
       "      <td>3.038254e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.853797</td>\n",
       "      <td>0.146177</td>\n",
       "      <td>2.590313e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.825383</td>\n",
       "      <td>0.174559</td>\n",
       "      <td>5.793567e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.897324</td>\n",
       "      <td>0.102665</td>\n",
       "      <td>1.120500e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.926987</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>1.296939e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.895065</td>\n",
       "      <td>0.104896</td>\n",
       "      <td>3.925062e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.861840</td>\n",
       "      <td>0.138141</td>\n",
       "      <td>1.890958e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.803157</td>\n",
       "      <td>0.196758</td>\n",
       "      <td>8.478611e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.795422</td>\n",
       "      <td>0.204553</td>\n",
       "      <td>2.568322e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.892083</td>\n",
       "      <td>0.107911</td>\n",
       "      <td>6.171769e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.863365</td>\n",
       "      <td>0.136601</td>\n",
       "      <td>3.442018e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.788178</td>\n",
       "      <td>0.211795</td>\n",
       "      <td>2.745268e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.835080</td>\n",
       "      <td>0.164888</td>\n",
       "      <td>3.214264e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.928350</td>\n",
       "      <td>0.071649</td>\n",
       "      <td>9.662549e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.964536</td>\n",
       "      <td>0.035462</td>\n",
       "      <td>2.258779e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.940906</td>\n",
       "      <td>0.059089</td>\n",
       "      <td>4.844218e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.891740</td>\n",
       "      <td>0.108246</td>\n",
       "      <td>1.417721e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.896526</td>\n",
       "      <td>0.103468</td>\n",
       "      <td>6.775673e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.923616</td>\n",
       "      <td>0.076373</td>\n",
       "      <td>1.182484e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.830668</td>\n",
       "      <td>0.169316</td>\n",
       "      <td>1.520937e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.921915</td>\n",
       "      <td>0.078068</td>\n",
       "      <td>1.783840e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.926585</td>\n",
       "      <td>0.073407</td>\n",
       "      <td>8.461627e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.867786</td>\n",
       "      <td>0.132146</td>\n",
       "      <td>6.819319e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.841272</td>\n",
       "      <td>0.158656</td>\n",
       "      <td>7.259031e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.777263</td>\n",
       "      <td>0.222695</td>\n",
       "      <td>4.153657e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.881389</td>\n",
       "      <td>0.118569</td>\n",
       "      <td>4.180758e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.869975</td>\n",
       "      <td>0.130014</td>\n",
       "      <td>1.157949e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.860034</td>\n",
       "      <td>0.139955</td>\n",
       "      <td>1.040830e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.832053</td>\n",
       "      <td>0.167893</td>\n",
       "      <td>5.416255e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.225293</td>\n",
       "      <td>7.742028e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.230143</td>\n",
       "      <td>7.685176e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.428007</td>\n",
       "      <td>5.719548e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.002053</td>\n",
       "      <td>0.400422</td>\n",
       "      <td>5.975251e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.237204</td>\n",
       "      <td>7.621181e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.397528</td>\n",
       "      <td>6.020159e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.383867</td>\n",
       "      <td>6.129345e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.003424</td>\n",
       "      <td>0.327541</td>\n",
       "      <td>6.690353e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.298289</td>\n",
       "      <td>7.014108e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.510705</td>\n",
       "      <td>4.886165e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.427942</td>\n",
       "      <td>5.718964e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.344845</td>\n",
       "      <td>6.545099e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.278027</td>\n",
       "      <td>7.216973e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.002077</td>\n",
       "      <td>0.490653</td>\n",
       "      <td>5.072700e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.442581</td>\n",
       "      <td>5.570645e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.342008</td>\n",
       "      <td>6.578098e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.128603</td>\n",
       "      <td>8.707666e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.320888</td>\n",
       "      <td>6.781900e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.004293</td>\n",
       "      <td>0.318426</td>\n",
       "      <td>6.772806e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.300990</td>\n",
       "      <td>6.978437e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.202462</td>\n",
       "      <td>7.970918e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.002152</td>\n",
       "      <td>0.248822</td>\n",
       "      <td>7.490253e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.294423</td>\n",
       "      <td>7.047682e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.224920</td>\n",
       "      <td>7.747891e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.153985</td>\n",
       "      <td>8.455648e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.233617</td>\n",
       "      <td>7.652262e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.379220</td>\n",
       "      <td>6.198606e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.298380</td>\n",
       "      <td>7.001622e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.131786</td>\n",
       "      <td>8.671166e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.281058</td>\n",
       "      <td>7.172582e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1             2\n",
       "0    0.879682  0.120308  1.081314e-05\n",
       "1    0.799706  0.200263  3.038254e-05\n",
       "2    0.853797  0.146177  2.590313e-05\n",
       "3    0.825383  0.174559  5.793567e-05\n",
       "4    0.897324  0.102665  1.120500e-05\n",
       "5    0.926987  0.073000  1.296939e-05\n",
       "6    0.895065  0.104896  3.925062e-05\n",
       "7    0.861840  0.138141  1.890958e-05\n",
       "8    0.803157  0.196758  8.478611e-05\n",
       "9    0.795422  0.204553  2.568322e-05\n",
       "10   0.892083  0.107911  6.171769e-06\n",
       "11   0.863365  0.136601  3.442018e-05\n",
       "12   0.788178  0.211795  2.745268e-05\n",
       "13   0.835080  0.164888  3.214264e-05\n",
       "14   0.928350  0.071649  9.662549e-07\n",
       "15   0.964536  0.035462  2.258779e-06\n",
       "16   0.940906  0.059089  4.844218e-06\n",
       "17   0.891740  0.108246  1.417721e-05\n",
       "18   0.896526  0.103468  6.775673e-06\n",
       "19   0.923616  0.076373  1.182484e-05\n",
       "20   0.830668  0.169316  1.520937e-05\n",
       "21   0.921915  0.078068  1.783840e-05\n",
       "22   0.926585  0.073407  8.461627e-06\n",
       "23   0.867786  0.132146  6.819319e-05\n",
       "24   0.841272  0.158656  7.259031e-05\n",
       "25   0.777263  0.222695  4.153657e-05\n",
       "26   0.881389  0.118569  4.180758e-05\n",
       "27   0.869975  0.130014  1.157949e-05\n",
       "28   0.860034  0.139955  1.040830e-05\n",
       "29   0.832053  0.167893  5.416255e-05\n",
       "..        ...       ...           ...\n",
       "120  0.000504  0.225293  7.742028e-01\n",
       "121  0.001339  0.230143  7.685176e-01\n",
       "122  0.000038  0.428007  5.719548e-01\n",
       "123  0.002053  0.400422  5.975251e-01\n",
       "124  0.000678  0.237204  7.621181e-01\n",
       "125  0.000456  0.397528  6.020159e-01\n",
       "126  0.003199  0.383867  6.129345e-01\n",
       "127  0.003424  0.327541  6.690353e-01\n",
       "128  0.000301  0.298289  7.014108e-01\n",
       "129  0.000678  0.510705  4.886165e-01\n",
       "130  0.000162  0.427942  5.718964e-01\n",
       "131  0.000645  0.344845  6.545099e-01\n",
       "132  0.000275  0.278027  7.216973e-01\n",
       "133  0.002077  0.490653  5.072700e-01\n",
       "134  0.000355  0.442581  5.570645e-01\n",
       "135  0.000182  0.342008  6.578098e-01\n",
       "136  0.000631  0.128603  8.707666e-01\n",
       "137  0.000922  0.320888  6.781900e-01\n",
       "138  0.004293  0.318426  6.772806e-01\n",
       "139  0.001167  0.300990  6.978437e-01\n",
       "140  0.000446  0.202462  7.970918e-01\n",
       "141  0.002152  0.248822  7.490253e-01\n",
       "142  0.000809  0.294423  7.047682e-01\n",
       "143  0.000291  0.224920  7.747891e-01\n",
       "144  0.000450  0.153985  8.455648e-01\n",
       "145  0.001157  0.233617  7.652262e-01\n",
       "146  0.000919  0.379220  6.198606e-01\n",
       "147  0.001458  0.298380  7.001622e-01\n",
       "148  0.001098  0.131786  8.671166e-01\n",
       "149  0.001684  0.281058  7.172582e-01\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction probabilities\n",
    "iris_predict_data = pd.DataFrame(logit_1.predict_proba(iris_X))\n",
    "iris_predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.128195</td>\n",
       "      <td>-2.117704</td>\n",
       "      <td>-11.434749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.223511</td>\n",
       "      <td>-1.608122</td>\n",
       "      <td>-10.401643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.158062</td>\n",
       "      <td>-1.922935</td>\n",
       "      <td>-10.561147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.191908</td>\n",
       "      <td>-1.745493</td>\n",
       "      <td>-9.756177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.108339</td>\n",
       "      <td>-2.276282</td>\n",
       "      <td>-11.399150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.075816</td>\n",
       "      <td>-2.617290</td>\n",
       "      <td>-11.252919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.110859</td>\n",
       "      <td>-2.254788</td>\n",
       "      <td>-10.145543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.148686</td>\n",
       "      <td>-1.979477</td>\n",
       "      <td>-10.875842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.219205</td>\n",
       "      <td>-1.625778</td>\n",
       "      <td>-9.375379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.228883</td>\n",
       "      <td>-1.586929</td>\n",
       "      <td>-10.569673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.114196</td>\n",
       "      <td>-2.226451</td>\n",
       "      <td>-11.995525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.146918</td>\n",
       "      <td>-1.990694</td>\n",
       "      <td>-10.276868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.238032</td>\n",
       "      <td>-1.552137</td>\n",
       "      <td>-10.503047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.180228</td>\n",
       "      <td>-1.802488</td>\n",
       "      <td>-10.345327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.074347</td>\n",
       "      <td>-2.635974</td>\n",
       "      <td>-13.849838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.036108</td>\n",
       "      <td>-3.339291</td>\n",
       "      <td>-13.000686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.060912</td>\n",
       "      <td>-2.828710</td>\n",
       "      <td>-12.237725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.114580</td>\n",
       "      <td>-2.223352</td>\n",
       "      <td>-11.163875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.109228</td>\n",
       "      <td>-2.268497</td>\n",
       "      <td>-11.902172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.079459</td>\n",
       "      <td>-2.572131</td>\n",
       "      <td>-11.345308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.185525</td>\n",
       "      <td>-1.775986</td>\n",
       "      <td>-11.093599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.081303</td>\n",
       "      <td>-2.550181</td>\n",
       "      <td>-10.934157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.076250</td>\n",
       "      <td>-2.611738</td>\n",
       "      <td>-11.679969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.141811</td>\n",
       "      <td>-2.023847</td>\n",
       "      <td>-9.593166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.172841</td>\n",
       "      <td>-1.841018</td>\n",
       "      <td>-9.530679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.251976</td>\n",
       "      <td>-1.501951</td>\n",
       "      <td>-10.088936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.126256</td>\n",
       "      <td>-2.132260</td>\n",
       "      <td>-10.082433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.139291</td>\n",
       "      <td>-2.040116</td>\n",
       "      <td>-11.366275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.150783</td>\n",
       "      <td>-1.966431</td>\n",
       "      <td>-11.472907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.183859</td>\n",
       "      <td>-1.784429</td>\n",
       "      <td>-9.823521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>-7.591987</td>\n",
       "      <td>-1.490355</td>\n",
       "      <td>-0.255921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>-6.615729</td>\n",
       "      <td>-1.469053</td>\n",
       "      <td>-0.263292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>-10.172421</td>\n",
       "      <td>-0.848616</td>\n",
       "      <td>-0.558695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-6.188457</td>\n",
       "      <td>-0.915237</td>\n",
       "      <td>-0.514959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>-7.296589</td>\n",
       "      <td>-1.438835</td>\n",
       "      <td>-0.271654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-7.692178</td>\n",
       "      <td>-0.922491</td>\n",
       "      <td>-0.507471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-5.745046</td>\n",
       "      <td>-0.957459</td>\n",
       "      <td>-0.489497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>-5.677051</td>\n",
       "      <td>-1.116142</td>\n",
       "      <td>-0.401919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>-8.109913</td>\n",
       "      <td>-1.209694</td>\n",
       "      <td>-0.354662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>-7.295808</td>\n",
       "      <td>-0.671963</td>\n",
       "      <td>-0.716177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>-8.729649</td>\n",
       "      <td>-0.848768</td>\n",
       "      <td>-0.558797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>-7.346608</td>\n",
       "      <td>-1.064659</td>\n",
       "      <td>-0.423869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>-8.197722</td>\n",
       "      <td>-1.280036</td>\n",
       "      <td>-0.326149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>-6.176679</td>\n",
       "      <td>-0.712019</td>\n",
       "      <td>-0.678712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>-7.944285</td>\n",
       "      <td>-0.815132</td>\n",
       "      <td>-0.585074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>-8.611407</td>\n",
       "      <td>-1.072921</td>\n",
       "      <td>-0.418839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>-7.368349</td>\n",
       "      <td>-2.051029</td>\n",
       "      <td>-0.138381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>-6.989030</td>\n",
       "      <td>-1.136663</td>\n",
       "      <td>-0.388328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>-5.450742</td>\n",
       "      <td>-1.144364</td>\n",
       "      <td>-0.389670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>-6.753485</td>\n",
       "      <td>-1.200680</td>\n",
       "      <td>-0.359760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>-7.714540</td>\n",
       "      <td>-1.597203</td>\n",
       "      <td>-0.226785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>-6.141230</td>\n",
       "      <td>-1.391016</td>\n",
       "      <td>-0.288983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>-7.119626</td>\n",
       "      <td>-1.222739</td>\n",
       "      <td>-0.349886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>-8.141629</td>\n",
       "      <td>-1.492012</td>\n",
       "      <td>-0.255164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>-7.705203</td>\n",
       "      <td>-1.870902</td>\n",
       "      <td>-0.167751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>-6.761711</td>\n",
       "      <td>-1.454074</td>\n",
       "      <td>-0.267584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>-6.992197</td>\n",
       "      <td>-0.969638</td>\n",
       "      <td>-0.478261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>-6.530609</td>\n",
       "      <td>-1.209388</td>\n",
       "      <td>-0.356443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>-6.814449</td>\n",
       "      <td>-2.026579</td>\n",
       "      <td>-0.142582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>-6.386598</td>\n",
       "      <td>-1.269195</td>\n",
       "      <td>-0.332319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1          2\n",
       "0    -0.128195 -2.117704 -11.434749\n",
       "1    -0.223511 -1.608122 -10.401643\n",
       "2    -0.158062 -1.922935 -10.561147\n",
       "3    -0.191908 -1.745493  -9.756177\n",
       "4    -0.108339 -2.276282 -11.399150\n",
       "5    -0.075816 -2.617290 -11.252919\n",
       "6    -0.110859 -2.254788 -10.145543\n",
       "7    -0.148686 -1.979477 -10.875842\n",
       "8    -0.219205 -1.625778  -9.375379\n",
       "9    -0.228883 -1.586929 -10.569673\n",
       "10   -0.114196 -2.226451 -11.995525\n",
       "11   -0.146918 -1.990694 -10.276868\n",
       "12   -0.238032 -1.552137 -10.503047\n",
       "13   -0.180228 -1.802488 -10.345327\n",
       "14   -0.074347 -2.635974 -13.849838\n",
       "15   -0.036108 -3.339291 -13.000686\n",
       "16   -0.060912 -2.828710 -12.237725\n",
       "17   -0.114580 -2.223352 -11.163875\n",
       "18   -0.109228 -2.268497 -11.902172\n",
       "19   -0.079459 -2.572131 -11.345308\n",
       "20   -0.185525 -1.775986 -11.093599\n",
       "21   -0.081303 -2.550181 -10.934157\n",
       "22   -0.076250 -2.611738 -11.679969\n",
       "23   -0.141811 -2.023847  -9.593166\n",
       "24   -0.172841 -1.841018  -9.530679\n",
       "25   -0.251976 -1.501951 -10.088936\n",
       "26   -0.126256 -2.132260 -10.082433\n",
       "27   -0.139291 -2.040116 -11.366275\n",
       "28   -0.150783 -1.966431 -11.472907\n",
       "29   -0.183859 -1.784429  -9.823521\n",
       "..         ...       ...        ...\n",
       "120  -7.591987 -1.490355  -0.255921\n",
       "121  -6.615729 -1.469053  -0.263292\n",
       "122 -10.172421 -0.848616  -0.558695\n",
       "123  -6.188457 -0.915237  -0.514959\n",
       "124  -7.296589 -1.438835  -0.271654\n",
       "125  -7.692178 -0.922491  -0.507471\n",
       "126  -5.745046 -0.957459  -0.489497\n",
       "127  -5.677051 -1.116142  -0.401919\n",
       "128  -8.109913 -1.209694  -0.354662\n",
       "129  -7.295808 -0.671963  -0.716177\n",
       "130  -8.729649 -0.848768  -0.558797\n",
       "131  -7.346608 -1.064659  -0.423869\n",
       "132  -8.197722 -1.280036  -0.326149\n",
       "133  -6.176679 -0.712019  -0.678712\n",
       "134  -7.944285 -0.815132  -0.585074\n",
       "135  -8.611407 -1.072921  -0.418839\n",
       "136  -7.368349 -2.051029  -0.138381\n",
       "137  -6.989030 -1.136663  -0.388328\n",
       "138  -5.450742 -1.144364  -0.389670\n",
       "139  -6.753485 -1.200680  -0.359760\n",
       "140  -7.714540 -1.597203  -0.226785\n",
       "141  -6.141230 -1.391016  -0.288983\n",
       "142  -7.119626 -1.222739  -0.349886\n",
       "143  -8.141629 -1.492012  -0.255164\n",
       "144  -7.705203 -1.870902  -0.167751\n",
       "145  -6.761711 -1.454074  -0.267584\n",
       "146  -6.992197 -0.969638  -0.478261\n",
       "147  -6.530609 -1.209388  -0.356443\n",
       "148  -6.814449 -2.026579  -0.142582\n",
       "149  -6.386598 -1.269195  -0.332319\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction of log odds\n",
    "iris_log_predict = pd.DataFrame(logit_1.predict_log_proba(iris_X))\n",
    "iris_log_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees and Tree based models are also in Sklearn. While decision tree and random forest are often taught together, it's important to remember that most tree based models are ensembles so they are imported from different areas in sklearn.\n",
    "As tree based models have lots of parameters that are reliant on theory of trees, I won't go too indepth into them here except to highlight some common parameters that are often tuned.\n",
    "Trees can be split along according to gini or entropy as I'm using a Classifier here. It's worth evaluating which metrics gives you better performance even though both attempt to do similar things. Regressor models have other criterias as well such as \"mse\", etc. Standard things such as max_depth, min_samples_split, min_samples_leaf, etc can all be tuned as well. Note that the max_features is set to \"None\" when generally the \"standard\" practice is set it to square root of your features. Both \"sqrt\" or \"auto\" will adjust this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_tree.fit(iris_X, iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01333333,  0.        ,  0.56405596,  0.42261071])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examining feature importances of your tree model\n",
    "iris_tree.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1b42b4da208>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAE7CAYAAADAcwqtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCaCHziALdgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b42b4c58d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#A neat way to visualize your feature importance!\n",
    "pd.Series(index = iris_feat_list, data = iris_tree.feature_importances_).sort_values().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examining different classes in your class target\n",
    "iris_tree.n_classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting how many features are in your model\n",
    "iris_tree.n_features_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, Sklearn's Random Forest and Gradient Boosting Machine are both under ensemble models. Most of the parameters are similar to tree's model itself so I'll simply be going over some ensemble specific parameters. Note you can also use VotingClassifier from this package as a simple ensemble object if you want to play with model ensembling a bit on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things to note is that since these are ensembles of decision trees, one new parameter is \"n_estimators\". This signifies the amount of trees you desire for your ensemble. Generally a larger number such as 1000-2000 is recommended but we'll use the default 10 for this toy example. Another thing worth noting is that max_features is set to \"auto\" to designate that it will use square root of total features when considering a split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_rf_1 = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_rf_1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This will show you the base estimator of your ensemble, in this case, a decision tree regressor\n",
    "boston_rf_1.base_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False,\n",
       "            random_state=47890994, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False,\n",
       "            random_state=1501403514, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False,\n",
       "            random_state=1295269591, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False,\n",
       "            random_state=679951101, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False,\n",
       "            random_state=609898273, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False,\n",
       "            random_state=1965624233, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False,\n",
       "            random_state=995529730, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False,\n",
       "            random_state=994659958, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False,\n",
       "            random_state=1763296318, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False,\n",
       "            random_state=1228648901, splitter='best')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This will show you a list of ALL the estimators in your model\n",
    "boston_rf_1.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26.36,  22.58,  36.48,  33.83,  35.62,  27.01,  21.69,  24.09,\n",
       "        18.12,  19.18,  16.48,  19.17,  21.12,  20.3 ,  18.88,  19.9 ,\n",
       "        21.94,  18.12,  20.08,  18.98,  13.88,  19.73,  14.74,  14.36,\n",
       "        16.39,  14.6 ,  16.49,  15.13,  19.45,  21.03,  13.34,  19.42,\n",
       "        13.53,  13.23,  13.75,  19.11,  19.64,  21.71,  23.19,  29.82,\n",
       "        35.28,  27.68,  25.15,  25.05,  21.73,  19.35,  20.03,  17.45,\n",
       "        17.4 ,  19.2 ,  21.6 ,  21.31,  24.53,  23.08,  18.88,  34.91,\n",
       "        23.56,  30.67,  23.54,  19.98,  18.97,  17.37,  23.75,  25.29,\n",
       "        32.37,  23.66,  19.56,  20.65,  17.92,  20.29,  23.83,  21.99,\n",
       "        22.84,  23.3 ,  24.49,  21.33,  19.88,  20.89,  20.94,  20.55,\n",
       "        27.16,  24.21,  24.03,  23.28,  22.97,  26.8 ,  21.7 ,  22.55,\n",
       "        25.37,  27.76,  22.51,  22.32,  22.88,  25.53,  20.98,  28.34,\n",
       "        21.68,  40.3 ,  42.78,  34.2 ,  26.3 ,  25.89,  18.02,  19.64,\n",
       "        20.1 ,  20.12,  19.28,  19.89,  19.75,  19.02,  21.09,  23.78,\n",
       "        18.94,  18.91,  19.61,  19.03,  20.94,  19.63,  19.57,  19.52,\n",
       "        21.75,  21.38,  20.23,  17.66,  18.8 ,  19.97,  15.85,  15.66,\n",
       "        18.01,  14.97,  19.16,  19.54,  22.11,  17.93,  15.65,  18.08,\n",
       "        17.47,  17.41,  13.21,  17.36,  13.99,  13.68,  14.2 ,  14.02,\n",
       "        12.36,  13.76,  15.27,  14.69,  17.14,  15.78,  21.69,  19.19,\n",
       "        18.06,  18.32,  17.56,  16.7 ,  13.74,  36.04,  25.  ,  25.08,\n",
       "        27.  ,  48.48,  49.45,  50.  ,  22.45,  24.49,  48.76,  22.98,\n",
       "        23.67,  22.8 ,  18.48,  20.88,  21.46,  23.23,  22.33,  27.4 ,\n",
       "        22.52,  24.37,  29.64,  33.36,  40.35,  34.88,  37.92,  31.33,\n",
       "        24.97,  28.63,  46.58,  30.88,  28.2 ,  34.97,  35.02,  30.13,\n",
       "        36.28,  30.79,  29.08,  49.08,  33.41,  30.32,  34.19,  34.02,\n",
       "        33.24,  23.82,  44.61,  47.72,  49.85,  22.52,  24.44,  20.61,\n",
       "        23.11,  18.84,  20.46,  18.8 ,  21.04,  26.44,  22.07,  24.56,\n",
       "        22.16,  26.83,  20.76,  22.82,  27.53,  19.47,  27.54,  27.87,\n",
       "        45.12,  44.53,  39.85,  32.12,  44.19,  31.  ,  24.04,  31.8 ,\n",
       "        44.69,  46.53,  28.14,  23.82,  26.02,  31.52,  23.79,  23.83,\n",
       "        24.02,  20.6 ,  22.5 ,  23.97,  17.37,  19.04,  23.64,  20.42,\n",
       "        23.81,  26.97,  24.82,  24.4 ,  30.93,  41.98,  22.47,  20.32,\n",
       "        44.29,  46.38,  36.33,  30.98,  31.85,  43.67,  49.16,  31.89,\n",
       "        34.72,  23.32,  26.98,  47.41,  43.78,  20.67,  20.65,  24.94,\n",
       "        24.92,  37.6 ,  32.21,  32.  ,  33.82,  32.  ,  27.22,  34.19,\n",
       "        44.43,  34.78,  44.78,  49.08,  31.3 ,  22.98,  19.98,  23.15,\n",
       "        22.54,  24.14,  30.42,  37.77,  28.98,  23.37,  21.84,  28.52,\n",
       "        26.23,  20.52,  22.76,  31.58,  27.76,  23.24,  24.48,  33.33,\n",
       "        34.64,  27.98,  34.61,  28.03,  24.03,  20.02,  16.92,  22.98,\n",
       "        19.65,  22.12,  23.07,  16.98,  17.44,  19.29,  22.86,  21.39,\n",
       "        23.5 ,  23.15,  19.86,  18.47,  24.4 ,  24.4 ,  23.4 ,  21.89,\n",
       "        20.04,  22.99,  20.64,  17.4 ,  20.06,  22.52,  21.15,  20.47,\n",
       "        19.29,  18.82,  20.42,  19.01,  18.9 ,  33.05,  21.66,  25.66,\n",
       "        30.92,  18.07,  18.41,  23.48,  25.07,  27.61,  23.  ,  24.47,\n",
       "        19.05,  30.16,  18.81,  20.81,  17.04,  21.04,  21.78,  21.9 ,\n",
       "        22.03,  20.23,  20.65,  17.43,  33.46,  26.81,  21.1 ,  23.23,\n",
       "        47.28,  46.41,  45.17,  50.  ,  50.  ,  13.1 ,  14.21,  21.74,\n",
       "        13.13,  14.75,  11.25,  10.19,  15.53,   9.87,  11.24,  10.95,\n",
       "         8.45,   7.94,  10.14,   7.94,   9.12,  10.83,  14.29,  20.92,\n",
       "        10.07,  13.91,  12.6 ,  15.17,  13.5 ,   9.92,   5.81,   8.41,\n",
       "         6.59,   8.51,  11.42,   9.81,   7.89,   6.54,  11.39,  33.66,\n",
       "        14.58,  25.35,  15.  ,  17.79,  16.12,  15.36,   8.85,   7.43,\n",
       "         8.24,   9.56,   9.42,  10.47,  16.02,  14.49,  21.52,  12.7 ,\n",
       "        12.08,   8.38,  11.03,  12.05,  11.8 ,   9.57,  14.24,  15.44,\n",
       "        18.98,  14.41,  11.9 ,  13.1 ,  10.12,   8.39,   8.32,  12.8 ,\n",
       "        10.41,  14.9 ,  17.98,  15.87,  11.  ,  10.41,  14.98,  13.99,\n",
       "        14.39,  14.04,  14.08,  16.65,  16.26,  16.12,  14.8 ,  13.83,\n",
       "        14.25,  15.7 ,  15.6 ,  19.66,  16.18,  18.48,  19.99,  21.78,\n",
       "        21.32,  20.69,  16.66,  17.38,  16.99,  20.1 ,  19.75,  20.17,\n",
       "        23.2 ,  28.48,  13.93,  14.65,  17.05,  12.37,  14.23,  21.2 ,\n",
       "        22.2 ,  24.72,  25.38,  21.25,  20.58,  21.58,  18.4 ,  20.83,\n",
       "        16.08,   7.45,   8.85,  15.38,  20.2 ,  20.25,  22.45,  22.06,\n",
       "        17.64,  18.79,  20.96,  17.82,  17.19,  22.76,  21.35,  27.99,\n",
       "        23.7 ,  14.39])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting with model\n",
    "boston_rf_1.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBM is the last model I'd like to discuss in this tutorial. There's a few new metrics to take into account here. One is the loss function that you wish to optimize and the other is the learning rate. I generally don't touch these but the learning_rate can be thought of as a recipricol effect with n_estimators. So more trees, you can use a faster learning rate and vice versa but tuning these can lead to better results. Subsample is another parameter that I often tune. I've COMPLETELY forgotten the mathematical reason behind it but general good practice is to leave it at .8 or so (depends on size of your data as well!) rather than the default 1.0 for SGD purposes. Once again, for simplicity sake, I will just leave most parameters at base though here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_gbm = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_gbm.fit(iris_X, iris_y)\n",
    "iris_gbm.predict(iris_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally there's another package in sklearn that's really useful which is the metrics package. For the sake of time, I don't have too much time to go into it but it's worth looking into. There's lots of different metrics there where you'll be able to evaluate such as RSME, accuracy, etc and the like. I'll show a standard convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the various metrics, simply call \"metrics.FUNCTION(true_y, predicted_y) and it'll often give you the proper score. If you have issues with this, feel free to find me. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#example: metrics.accuracy_score(true_y, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to cross validate your model, in Python 3, you'll need to import it from the model_selection module in sklearn. You can also use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we will 5-fold cross validate the linear regression model from earlier as well as store the scores \n",
    "into the scores variable. You can adjust the scoring metrics as well but I'm leaving it default for now. Note the cross_validate function is imported here as well to show that you can also create an object that allows you to cross validate across multiple different scoring metrics and parameters. For simplicity sake, it won't be demo'd here but it's useful to note if you feel more comfortable with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.63861069,  0.71334432,  0.58645134,  0.07842495, -0.26312455])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(linreg_1, X, y, cv = 5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a holdout set using the train_test_split module from model_selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54426980267487357"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n",
    "clf = linreg_1.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a demo of the example as well as how to make a cross validated prediction from the training set to predict the hold out set. You can then use the metrics function mentioned previously as well on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55965243464551184"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = cross_val_predict(linreg_1, X, y, cv = 5)\n",
    "metrics.r2_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
