{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: A general introduction to basic NLP methods to serve as a foundation for further self study and additional NLP curriculums. This notebook intends to serve as a basis for various techniques such as Text Processing, vectorizing text, information retrieval, named entity recognition, and classifying text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing Steps and Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can make use of any text in machine assisted study of text or maching learning methods dealing with text, it's important that the text is fed as an input the model can interpret. To this end, the specific steps one must take is:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization\n",
    "- Filtering\n",
    "- Lemmitization\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the act of splitting one's text into a sequence of words. The most common method of tokenization is the \"bag of words\" approach but it's important to recognize bag of words is one method of tokenization rather than a lateral method. Certain tokenization methods will preserve the sequential information of text while other methods such as \"bag of words\" will not. In this example, we will try a simplistic tokenization method below using the standard split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'toy', 'example.', 'Illustrate', 'this', 'example', 'below.']\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"This is a toy example. Illustrate this example below.\"\n",
    "sample_tokens = sample_text.split()\n",
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice something? While we have the tokens, \"example\" and \"example.\" are treated as different tokens. As a NLP data scientist, you must make the choice on whether you choose to distinguish the two.\n",
    "\n",
    "Note, various packages in Python such as the nltk package will default tokenize \".\" as a seperate token instead to designate it it's own special meaning. This can be illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'toy',\n",
       " 'example',\n",
       " '.',\n",
       " 'Illustrate',\n",
       " 'this',\n",
       " 'example',\n",
       " 'below',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.word_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When examining a text, often there are words used within a sentence that holds no meaning for various data mining such as topic modeling or word frequency. Examples of this include \"the\", \"is\", etc. Collectively, these are known as \"stopwords\". When mining for certain information, you should note whether your method should remove certain stopwords (for example, wordclouds) or keep them (ex. bigrams). Tools for removing stopwords are in most nlp packages but we will use a self created method to demonstrate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toy example. Illustrate example below.\n"
     ]
    }
   ],
   "source": [
    "sample_text2 = \"This is a toy example. Illustrate this example below.\"\n",
    "stopWords = ['this', 'a', 'is']\n",
    "sample_words = sample_text2.split()\n",
    "\n",
    "results  = ' '.join([word for word in sample_words if word.lower() not in stopWords])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To illustrate an example, we will call upon the stopwords method from nltk. Note, methods that interact with the text itself is usually found under nltk.corpus. Corpus is the linguistics term for set of structured text used for statistical study so be mindful of this specific vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'o', u'hadn', u'herself', u'll', u'had', u'should', u'to', u'only', u'won', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'd', u'did', u'didn', u'this', u'she', u'each', u'further', u'where', u'few', u'because', u'doing', u'some', u'hasn', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'while', u're', u'does', u'above', u'between', u'mustn', u't', u'be', u'we', u'who', u'were', u'here', u'shouldn', u'hers', u'by', u'on', u'about', u'couldn', u'of', u'against', u's', u'isn', u'or', u'own', u'into', u'yourself', u'down', u'mightn', u'wasn', u'your', u'from', u'her', u'their', u'aren', u'there', u'been', u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', u'himself', u'that', u'but', u'don', u'with', u'than', u'those', u'he', u'me', u'myself', u'ma', u'these', u'up', u'will', u'below', u'ain', u'can', u'theirs', u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', u'other', u'which', u'you', u'shan', u'needn', u'haven', u'after', u'most', u'such', u'why', u'a', u'off', u'i', u'm', u'yours', u'so', u'y', u'the', u'having', u'once'])\n"
     ]
    }
   ],
   "source": [
    "NLTKstopWords = set(stopwords.words('english'))\n",
    "print(NLTKstopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note one can add to this list of stopwords if one wishes to include various stopwords that are too common within the topic of text. For example \"computer\" would be a stopword in corpus largely dealing with data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various words in English have the same meaning. There are two main methods for handling tasks such as recognizing \"strike, striking, struck\" as the same words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming attempts to remove and replace the suffix of various words to attempt to reach the same root word.\n",
    "\n",
    "The most common stemming algorithms are:\n",
    "\n",
    "https://tartarus.org/martin/PorterStemmer/ (the older traditional method)\n",
    "\n",
    "and\n",
    "\n",
    "http://www.lancaster.ac.uk/scc/ (a more aggressive modern stemmer)\n",
    "\n",
    "Stemming and lemmatization can both be done with self written rules using creative forms of regex (and I urge you to try on simpler text for research purposes) but for practical example demo in this notebook, we will implement the PorterStemmer method from nltk on the example below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonprocess_text = \"I am writing a Python string\"\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmed_text = ' '.join([stemmer.stem(word) for word in nonprocess_text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am write a python string\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note! This is more robust than the standard regex implementation as we see here \"writing\" is converted to \"write\" but \"string\" isn't converted to \"stre\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unlike stemming, lemmatization will try to identify root words that are semantically similar to text based off a dictionary corpus. In essence, you can think of being able to replicate the effect manually by implementing a look-up method after parsing a text. There are various dictionaries one can use to base lemmization off of. NLTK's [wordnet](http://wordnet.princeton.edu/) is quite powerful to handle most lemmatization task. We'll examine a few implementations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Drace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('wordnet')\n",
    "\n",
    "#the prior line is a one time download IF wordnet is not already in your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemztr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'foot'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemztr.lemmatize('feet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, lemmatization will return back the default if the text isn't found in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abacadabradoo'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemztr.lemmatize('abacadabradoo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While on surface, lemmatizing seems to be the superior method to stemming, one must recognize that in most industry deployed example, stemming is often more practical. This is because, once the text is fed as a vectorized input, the algorithm doesn't mind whether the text states \"strike\" or \"strik\" as the text is read based on various methods of the algorithm itself rather than semantical understanding. Stemming is computationally more efficient and cheaper than lemmatization without much deviation in accuracy. That being said, lemmatizing should be considered when one desires output of processed text for semantical analysis or word looking up (for example, wordclouds, using it for field labels, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have examined methods for preprocessing text, let's examine methods of converting text to vectors of the modeling stage. Within NLP, there are two predominant methods: Frequency based and Prediction based.\n",
    "\n",
    "When you hear of \"bag-of-words\" or \"TF-IDF\", it is referring to Frequency based while \"word2vec\" or \"doc2vec\" refer to prediction based. Note, NEITHER of these embeddings are your actual classifiers/regressors but instead used to understand text behavior or pass as inputs into a predictive algorithm.\n",
    "\n",
    "For purposes of this tutorial, we will focus on the Frequency based methods as the others often uses pre-trained neural network models and as a result, has less use for understanding of foundational NLP methods. Since Frequency based methods lean traditionally on the bag-of-words method, it doesn't capture positions or semantics. However, despite this, the output vectors of these models can perform well in a variety of NLP predictive problems. (ex. a high frequency of \"free\", despite various positional text, generally would indicate an email is spam in the classic example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer & TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often this method is often referred to as \"bag-of-words\" but Countvectorizer can includes methods of n-grams to handle more complex tasks. At it's basic principle, it's simply a count of words throughout the document. \n",
    "\n",
    "TF-IDF goes one step further. As the acronym suggests (Term Frequency times Inverse Document Frequency), it applies a frequency count but then penalizes it by dividing it across the appearance throughout all documents.\n",
    "\n",
    "Often, these methods will create a gigantic  scipy sparse matrix object. The mathematics of joining a sparse matrix back with a standard matrix won't be discussed here but implementation of the code is provided below. For the purpose of this tutorial, we will work through sklearn's documentation examples but edit certain areas to suit the length of this exercise and also to hone on aspects for this example.\n",
    "\n",
    "We'll select the same four groups as per the tutorial and load a native dataset from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"sklearn.datasets.twenty_newsgroups\"\n"
     ]
    }
   ],
   "source": [
    "data = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this data set will be a huge dictionary as per standard of sklearn datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not subset part of the data for our use and set random_state to the same as per documentation. This allows you to easily examine results if you choose to follow the remainder of the sklearn's excellent tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "\n",
    "train_text = fetch_20newsgroups(subset= 'train', categories= cat, shuffle= True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note train_text.data will be a list of values. The same method and code structure will work if you pass the column of text from your pandas dataframe as well. Let's walk through how to transform the data through CountVectorization and TF-IDF respectively. Also important, this deviates from sklearn's tutorial a bit as one can use the TfidfTransformer method to transform the matrix created from CountVectorizer. We will be applying the word embedding twice on our end instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer()\n",
    "X_train_count = count_vec.fit_transform(train_text.data)\n",
    "#X_train_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer()\n",
    "X_train_tf = tf_idf.fit_transform(train_text.data)\n",
    "#X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz: given both methods, will the shape of the resulting matrix be the same as each other or different? Try and see yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything - Predicting Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've examined how to prepare text, transform text, we can examine various methods of predicting with our matrix. The traditional model for text predictions is Naive Bayes.\n",
    "\n",
    "Naive Bayes assumes that terms within documents are independent of each other. The classic example of this is the email spam detection. Consider implementation of this model on small amounts of texts across each document (ie reviews for negative, positive.) It’s important to note, because of the assumption that the terms would be independent of each other, removing highly correlated features before running the model would improve performance greatly.\n",
    "\n",
    "There are two prevailing models:\n",
    "The Multivariate Bernoulli model which ignores the frequency of words and Multinomial model which takes the frequency into account. Since we are implementing frequency rather than Boolean, it should be apparent that we will be using the Multinomial model.\n",
    "\n",
    "Note, there are many more methods for text prediction depending on tasks at hand and available resources such as kNN, Decision Trees, and SVM (which performs very well given appropriate resources)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a mock example of checking what are predicted tags using both sets of inputs.\n",
    "\n",
    "First we will create and train the model on the two training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multiNB = MultinomialNB()\n",
    "\n",
    "cntvecMNB = multiNB.fit(X_train_count, train_text.target)\n",
    "tf_idfMNB = multiNB.fit(X_train_tf, train_text.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the models are trained, we will send new statements using the same CountVectorizer and TF-IDF modeled we trained earlier to transform our new statements as new test statements and predict those new vectors with our Multinomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_docs = [\"\"\"In the ancient and medieval world  \n",
    "the etymological Latin root religio was understood as an individual virtue of worship \n",
    "never as doctrine, practice, or actual source of knowledge.  \n",
    "Furthermore, religio referred to broad social obligations to family, neighbors, rulers, and even towards God. \n",
    "When religio came into English around the 1200s as religion, it took the meaning of \"life bound by monastic vows\". \n",
    "The compartmentalized concept of religion, where religious things were separated from worldly things, \n",
    "            was not used before the 1500s. The concept of religion was first used in the 1500s to distinguish \n",
    "            the domain of the church and the domain of civil authorities.\"\"\",\n",
    "           \"\"\"A graphics processing unit (GPU) is a specialized electronic circuit designed to rapidly manipulate and \n",
    "           alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. \n",
    "           GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles. \n",
    "           Modern GPUs are very efficient at manipulating computer graphics and image processing, \n",
    "           and their highly parallel structure makes them more efficient than general-purpose CPUs  \n",
    "           for algorithms where the processing of large blocks of data is done in parallel. \n",
    "           In a personal computer, a GPU can be present on a video card, or it can be embedded \n",
    "           on the motherboard or—in certain CPUs—on the CPU die\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_doc_count = count_vec.transform(new_docs)\n",
    "new_doc_tfidf = tf_idf.transform(new_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnt_predicted = cntvecMNB.predict(new_doc_count)\n",
    "tfidf_predicted = tf_idfMNB.predict(new_doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soc.religion.christian\n",
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "for i in cnt_predicted:\n",
    "    print(train_text.target_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soc.religion.christian\n",
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "for i in tfidf_predicted:\n",
    "    print(train_text.target_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "So here we see both predictions accurately label the two but let's evaluate the behavior from an accuracy metrics of the two word embedding inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to see if you can walk through the steps you need to do before reading the code! Important: We do NOT fit the test data through a new CountVectorizer model or a new TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_text = fetch_20newsgroups(subset='test', categories=cat, shuffle=True, random_state= 42)\n",
    "test_docs = test_text.data\n",
    "\n",
    "test_docs_cV = count_vec.transform(test_docs)\n",
    "test_docs_tF = tf_idf.transform(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countvec_predicted = cntvecMNB.predict(test_docs_cV)\n",
    "tfidf_predicted = tf_idfMNB.predict(test_docs_tF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate a classification report to examine how our models perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The report for CountVectorizer word embedding through a Multinomial model:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.34      0.50       319\n",
      "         comp.graphics       0.97      0.58      0.73       389\n",
      "               sci.med       0.94      0.45      0.61       396\n",
      "soc.religion.christian       0.41      1.00      0.58       398\n",
      "\n",
      "           avg / total       0.81      0.61      0.61      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The report for CountVectorizer word embedding through a Multinomial model:')\n",
    "print(metrics.classification_report(test_text.target, countvec_predicted, target_names= test_text.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The report for TF-IDF word embedding through a Multinomial model:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.97      0.60      0.74       319\n",
      "         comp.graphics       0.96      0.89      0.92       389\n",
      "               sci.med       0.97      0.81      0.88       396\n",
      "soc.religion.christian       0.65      0.99      0.78       398\n",
      "\n",
      "           avg / total       0.88      0.83      0.84      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The report for TF-IDF word embedding through a Multinomial model:')\n",
    "print(metrics.classification_report(test_text.target, tfidf_predicted, target_names= test_text.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that TF-IDF performs significantly better in recall. Try to examine why this may be the case now knowing what you learned about the difference between CountVectorizer and TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider instances where CountVectorizer may work better than TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval\n",
    "\n",
    "Information retrieval plays a part in many areas of NLP. For example, even a ctrl + F serves as a very basic information retrieval as do many standard queries. The expanse of the various methods go beyond the depth of this notebook so we will examine one of the simpler techniques to help understand information retrieval as a concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Boolean Retrieval](https://en.wikipedia.org/wiki/Standard_Boolean_model)\n",
    "\n",
    "\n",
    "This method examines documents for occurance of terms. Counts are ignored and you must set rules for what terms are desired or NOT desired. For example, \"Google\" but NOT \"Facebook\". Take the boolean vectors of desired and the complement of NOT desired and evaluate the intersection of the boolean vectors. The resulting vector can then be used in the Boolean Retrieval Model to retrieve the documents that contains the vector representation.\n",
    "\n",
    "The initial input you will generate for a Boolean Retrieval is a Term Frequency matrix of occurance and absence.\n",
    "\n",
    "A dummy example is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dummy_boolean = pd.DataFrame({'doc1': [1, 0, 0], 'doc2': [0, 1, 0], 'doc3': [0,1,1]})\n",
    "dummy_boolean.index = ['hello', 'I', 'bye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>doc3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hello</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bye</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc1  doc2  doc3\n",
       "hello     1     0     0\n",
       "I         0     1     1\n",
       "bye       0     0     1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_boolean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we evaluate that \"hello\" occurs in doc1, \"I\" in doc2 and doc3, and \"bye\" in doc3. Note, this should look like the output of other objects you've just worked with in the notebook.\n",
    "\n",
    "If you thought this looks just like CountVectorizer, you're correct! You can simply CountVectorizer to create this matrix by setting any value >0 to 1. Boolean Retrieval does suffer dimensionality issues much like other frequency word embeddings so be mindful of this limitation.\n",
    "\n",
    "Once this is created, a Boolean Retrieval will now attempt to look for a vector of Boolean values that fit the rules you desire. For example, the boolean vector of 'I' and 'bye' would look like 001 while 'hello' and 'bye' would be 000.\n",
    "\n",
    "You can then apply these boolean vectors to text to query for any documents that fit the parameters you desire.\n",
    "\n",
    "Note, Boolean Retrieval isn't built into NLTK as there are more modern informational retrieval methods such as cosine similarity but I encourage you to practice building out your own Boolean Retrieval model if you are vested in studying NLP further. It's a great exercise that you can review with Stanford's NLP resource linked in the recommended reading below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is a powerful tool used a various tasks ranging from classification of texts to enhancing recommendation engines. One of the more powerful packages that has NER tools is [spaCy](https://spacy.io/) but for this specific topic we will not elect to use any packages but discuss some basic methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Gazetter](http://www.cogprints.org/5025/1/NRC-48727.pdf)\n",
    "\n",
    "Effectively the Gazetter acts as a dictionary look up. It works well for limited data sets (academic papers) but will fail as size of dataset increases . Terms are recognized from a precompiled set of lists which includes the names of entities. For example, “cake”, “pie”, “cookie”, would be tossed a gazette as “pastries”. It will then treat all “cakes, pies, cookies” as “pastries” and detach the meaning of “cake”, “pie”, “cookie” entirely.  Then you can classify say whether something is a baking recipe by using recipes to pass into gazettes to grab features that are in “measurement”, “tastes”, “pastries” balanced by other features to try to minimize false positives “whisk as a verb”, “fraction measurements”, etc.\n",
    "\n",
    "\n",
    "As a challenge, for practice, try to build a gazetter that will classify a dummy problem such as \"spam\" vs \"not spam\". Don't stress about the accuracy but try to aim to be as robust with your dictionary. Since the Gazetter functions similar to a query, you can easily pass some emails into your Gazetter to examine which ones it catches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Based Annotation\n",
    "\n",
    "While it's tempting to use more black box methods such as deep learning models, one must recognize that certain tasks in NLP requires a close adherence to how language is actually used. The classic example is \"Frederick\" which serves both as name of a person or name of a city. While more advanced models can apply methods such as RNN, rule based annotations are fast to deploy and easy to interpret.\n",
    "\n",
    "Advantages of this system is that if text falls under understood patterns, then you can mine easy tokens/meanings. Classic examples include rules for FIRSTNAME could be “First letter capital followed by “second word with capitalized first letter”/”verb”.” Disadvantages is scaling, requirement of rules being known prior, and application of domain expertise (back to FIRSTNAME example, avoiding ambiguity of FIRSTNAME and LOCATION)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "To complete the chapter on NLP, we still must discuss one of the most important areas in text mining, which is topic modeling. The most popular method of topic modeling in the most recent times is the [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) model or LDA.\n",
    "\n",
    "Gensim mentioned below has a great implementation of LDA that runs fast and allows saving of the model for future use. The code for implementing an lda model from gensim is simple but you should note the difficulty of fitting a LDA model is tuning the parameters for your specific documents. A sample code from a corpus I trained a prior model on is found below:\n",
    "\n",
    "NOTE THE FOLLOWING CODE WILL NOT RUN AS THERE ARE NO DOCUMENTS INSERTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading the requisite modules\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string,preprocess_documents\n",
    "from gensim import corpora,models, similarities\n",
    "\n",
    "#preprocessing documents saved under variable: texts\n",
    "processed_docs = preprocess_documents(texts)\n",
    "\n",
    "#creating dictionary and saving for future use\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "dictionary.save('text.dict')\n",
    "\n",
    "#Converting the corpus to a Matrix Market format, note that there are other formats!\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_docs]\n",
    "corpora.MmCorpus.serialize('text.mm', corpus)\n",
    "\n",
    "# tuning some parameters. Note the default of alpha is 'symmetric' \n",
    "# but I had adjusted it for my documents due to the nature of that task this notebook was drawn from.\n",
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=50, iterations= 500, alpha= 'asymmetric')\n",
    "lda.save('lda.model')\n",
    "\n",
    "# A visualization package that's very handy for visualizing your topics from your LDA model.\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(lda, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Recommended Resources:\n",
    "\n",
    "Libraries in Python:\n",
    "[NLTK](http://www.nltk.org/)\n",
    "\n",
    "[Gensim](https://radimrehurek.com/gensim/)\n",
    "\n",
    "[Stanford's NLP book](https://nlp.stanford.edu/IR-book/html/htmledition/contents-1.html)\n",
    "\n",
    "[Paper Used for this Study](https://nlp.stanford.edu/IR-book/html/htmledition/contents-1.html)\n",
    "\n",
    "[A Good Article on NER](http://www.datacommunitydc.org/blog/2013/04/a-survey-of-stochastic-and-gazetteer-based-approaches-for-named-entity-recognition-part-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
